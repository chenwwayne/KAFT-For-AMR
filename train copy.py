import math
import warnings
from typing import Dict, Tuple

import delu  # Deep Learning Utilities
import numpy as np
import time
import torch
import os
import gc
import argparse
from torch.amp import GradScaler, autocast
from torch.optim.lr_scheduler import ReduceLROnPlateau

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

import torch.nn.functional as F
import torch.optim
from torch import Tensor
from tqdm.std import tqdm
from thop import profile, clever_format
import sklearn.metrics
from typing import Callable, Tuple, Dict

from rtdl_revisiting_models import FTTransformer

warnings.simplefilter("ignore")
warnings.resetwarnings()

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

delu.random.seed(0)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(0)


def reshape_data(X_train: np.ndarray, X_val: np.ndarray, X_test: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """å°†æ•°æ®å±•å¹³ä¸º (n_samples, 256)"""
    X_train_reshaped = X_train.reshape(X_train.shape[0], -1)
    X_val_reshaped = X_val.reshape(X_val.shape[0], -1)
    X_test_reshaped = X_test.reshape(X_test.shape[0], -1)
    
    return X_train_reshaped, X_val_reshaped, X_test_reshaped

def load_and_preprocess_data(load_data_fn: Callable[[str], Tuple], data_path: str, batch_size=1, data_ratio=1.0):
    """
    åŠ è½½å¹¶é¢„å¤„ç† RML2016.10a æ•°æ®é›†ï¼Œä½¿ç”¨ One-hot ç¼–ç ã€‚
    é¢å¤–è¿”å› (mods, snrs, lbl, test_idx) æ–¹ä¾¿åç»­åœ¨ evaluate_by_snr ä½¿ç”¨ã€‚
    
    å‚æ•°:
        data_path: æ•°æ®é›†è·¯å¾„
        data_ratio: è¦ä½¿ç”¨çš„æ•°æ®æ¯”ä¾‹ (0.0-1.0)
    """
    # ç¬¬äº”ä¸ªè¿”å›å€¼æ˜¯ (train_idx, val_idx, test_idx) çš„å…ƒç»„
    (mods, snrs, lbl), (X_train, Y_train), (X_val, Y_val), (X_test, Y_test), (train_idx, val_idx, test_idx) = \
        load_data_fn(data_path)

    # æŒ‰æ¯”ä¾‹æˆªå–æ•°æ®
    if data_ratio < 1.0:
        train_size = int(len(X_train) * data_ratio)
        val_size = int(len(X_val) * data_ratio)
        test_size = int(len(X_test) * data_ratio)
        
        X_train = X_train[:train_size]
        Y_train = Y_train[:train_size]
        train_idx = train_idx[:train_size]
        
        X_val = X_val[:val_size]
        Y_val = Y_val[:val_size]
        val_idx = val_idx[:val_size]
        
        X_test = X_test[:test_size]
        Y_test = Y_test[:test_size]
        test_idx = test_idx[:test_size]

    # å±•å¹³
    X_train_reshaped, X_val_reshaped, X_test_reshaped = reshape_data(X_train, X_val, X_test)

    print("After reshaping and sampling:")
    print(f"X_train: {X_train_reshaped.shape} (sampled from original)")
    print(f"X_val:   {X_val_reshaped.shape} (sampled from original)")
    print(f"X_test:  {X_test_reshaped.shape} (sampled from original)")
    print(f"Y_train: {Y_train.shape}")

    n_classes = Y_train.shape[1]

    data = {
        "train": {
            "x": torch.as_tensor(X_train_reshaped, dtype=torch.float32),
            "y": torch.as_tensor(np.argmax(Y_train, axis=1), dtype=torch.long)
        },
        "val": {
            "x": torch.as_tensor(X_val_reshaped, dtype=torch.float32),
            "y": torch.as_tensor(np.argmax(Y_val, axis=1), dtype=torch.long)
        },
        "test": {
            "x": torch.as_tensor(X_test_reshaped, dtype=torch.float32),
            "y": torch.as_tensor(np.argmax(Y_test, axis=1), dtype=torch.long)
        }
    }
    n_cont_features = X_train_reshaped.shape[1]  # 256

    return data, n_classes, n_cont_features, mods, snrs, lbl, test_idx


def apply_model(model, batch: Dict[str, Tensor]) -> Tensor:
    """åº”ç”¨æ¨¡å‹"""
    x = batch["x"].to(device)
    output = model(x)
    return output

def define_model(n_cont_features: int, n_classes: int, args):
    """å®šä¹‰æ¨¡å‹"""
    model = FTTransformer(
        n_cont_features=n_cont_features,  # 256
        d_out=n_classes,
        **FTTransformer.get_default_kwargs(),
    ).to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)  # ä½¿ç”¨ä¼ å…¥çš„å­¦ä¹ ç‡
    # æ·»åŠ å­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer,
        mode='max',       # ç›‘æ§éªŒè¯é›†å‡†ç¡®ç‡
        factor= args.lr_factor,
        patience= args.lr_patience,
        verbose=True
    )
    
    return model, optimizer, scheduler  # ç°åœ¨è¿”å›ä¸‰ä¸ªå¯¹è±¡

# def train_model(model, optimizer, scheduler, data, n_epochs, patience, batch_size, accumulation_steps=1, lr=1e-3):
#     """è®­ç»ƒæ¨¡å‹"""
#     loss_fn = torch.nn.CrossEntropyLoss()  # ä½¿ç”¨å¤šåˆ†ç±»æŸå¤±å‡½æ•°
#     scaler = GradScaler()

#     epoch_size = math.ceil(len(data["train"]["y"]) / batch_size)
#     timer = delu.tools.Timer()
#     early_stopping = delu.tools.EarlyStopping(patience, mode="max")
#     best = {"val": -math.inf, "test": -math.inf, "epoch": -1}

#     start_time = time.time()
#     best_time = None

#     print(f"Device: {device.type.upper()}")
#     print("-" * 88 + "\n")
#     timer.run()

#     optimizer.zero_grad()

#     for epoch in range(n_epochs):
#         train_loss = 0.0
#         model.train()

#         for i, batch_cpu in enumerate(tqdm(delu.iter_batches(data["train"], batch_size, shuffle=True), desc=f"Epoch {epoch}", total=epoch_size)):
#             batch = {k: v.to(device) for k, v in batch_cpu.items()}

#             with autocast('cuda'):
#                 output = apply_model(model, batch)
#                 target = batch["y"]
#                 loss = loss_fn(output, target) / accumulation_steps

#             scaler.scale(loss).backward()

#             if (i + 1) % accumulation_steps == 0:
#                 scaler.step(optimizer)
#                 scaler.update()
#                 optimizer.zero_grad()

#             train_loss += loss.item() * len(batch["y"]) * accumulation_steps

#         train_loss /= len(data["train"]["y"])

#         model.eval()
#         val_loss = 0.0
#         test_loss = 0.0
#         with torch.no_grad(), autocast('cuda'):
#             for part in ["val", "test"]:
#                 loss_sum = 0.0
#                 for batch_cpu in delu.iter_batches(data[part], batch_size):
#                     batch = {k: v.to(device) for k, v in batch_cpu.items()}
#                     output = apply_model(model, batch)
#                     target = batch["y"]
#                     loss = loss_fn(output, target)
#                     loss_sum += loss.item() * len(batch["y"])
#                 if part == "val":
#                     val_loss = loss_sum / len(data[part]["y"])
#                 elif part == "test":
#                     test_loss = loss_sum / len(data[part]["y"])

#         val_score = evaluate(model, data, "val", batch_size=batch_size)[0]  # åªå– score
#         test_score = evaluate(model, data, "test", batch_size=batch_size)[0]  # åªå– score

#         scheduler.step(val_score)

#         print(
#             f"Epoch {epoch}, "
#             f"Train Loss: {train_loss:.4f}, "
#             f"Val Loss: {val_loss:.4f}, "
#             f"Test Loss: {test_loss:.4f}, "
#             f"Val Acc: {val_score:.4f}, "
#             f"Test Acc: {test_score:.4f}"
#         )

#         early_stopping.update(val_score)
#         if early_stopping.should_stop():
#             break

#         if val_score > best["val"]:
#             print("ğŸŒ¸ New best epoch! ğŸŒ¸")
#             best = {"val": val_score, "test": test_score, "epoch": epoch}
#             best_time = time.time()

#         print()

#         if best_time is not None:
#             total_time = best_time - start_time
#         print(f"Time to best model: {total_time:.2f} seconds")
#     else:
#         print("No best model found.")

#     return best

def train_model(model, optimizer, scheduler, data, n_epochs, patience, batch_size, accumulation_steps=1):
    """è®­ç»ƒæ¨¡å‹"""
    loss_fn = torch.nn.CrossEntropyLoss()  # ä½¿ç”¨å¤šåˆ†ç±»æŸå¤±å‡½æ•°
    scaler = GradScaler()

    epoch_size = math.ceil(len(data["train"]["y"]) / batch_size)
    timer = delu.tools.Timer()
    early_stopping = delu.tools.EarlyStopping(patience, mode="max")
    best = {"val": -math.inf, "test": -math.inf, "epoch": -1}

    start_time = time.time()
    best_time = None

    print(f"Device: {device.type.upper()}")
    print("-" * 88 + "\n")
    timer.run()

    optimizer.zero_grad()

    for epoch in range(n_epochs):
        train_loss = 0.0
        model.train()

        for i, batch_cpu in enumerate(tqdm(delu.iter_batches(data["train"], batch_size, shuffle=True), desc=f"Epoch {epoch}", total=epoch_size)):
            batch = {k: v.to(device) for k, v in batch_cpu.items()}

            with autocast('cuda'):
                output = apply_model(model, batch)
                # target = batch["y"].argmax(dim=1)  # å°†ç‹¬çƒ­ç¼–ç è½¬æ¢ä¸ºç±»åˆ«ç´¢å¼•
                target = batch["y"]
                loss = loss_fn(output, target) / accumulation_steps

            scaler.scale(loss).backward()

            if (i + 1) % accumulation_steps == 0:
                scaler.step(optimizer)
                scaler.update()
                optimizer.zero_grad()

            train_loss += loss.item() * len(batch["y"]) * accumulation_steps

        train_loss /= len(data["train"]["y"])

        model.eval()
        val_loss = 0.0
        test_loss = 0.0
        with torch.no_grad(), autocast('cuda'):
            for part in ["val", "test"]:
                loss_sum = 0.0
                for batch_cpu in delu.iter_batches(data[part], batch_size):
                    batch = {k: v.to(device) for k, v in batch_cpu.items()}
                    output = apply_model(model, batch)
                    # target = batch["y"].argmax(dim=1)  # å°†ç‹¬çƒ­ç¼–ç è½¬æ¢ä¸ºç±»åˆ«ç´¢å¼•
                    target = batch["y"]
                    loss = loss_fn(output, target)
                    loss_sum += loss.item() * len(batch["y"])
                if part == "val":
                    val_loss = loss_sum / len(data[part]["y"])
                elif part == "test":
                    test_loss = loss_sum / len(data[part]["y"])

        val_score = evaluate(model, data, "val", batch_size=batch_size)[0]  # åªå– score
        test_score = evaluate(model, data, "test", batch_size=batch_size)[0]  # åªå– score

        # # æ›´æ–°å­¦ä¹ ç‡
        # if isinstance(scheduler, ReduceLROnPlateau):
        #     scheduler.step(val_loss)
        # else:
        #     scheduler.step(val_score)

        scheduler.step(val_score)

        print(
            f"Epoch {epoch}, "
            f"Train Loss: {train_loss:.4f}, "
            f"Val Loss: {val_loss:.4f}, "
            f"Test Loss: {test_loss:.4f}, "
            f"Val Acc: {val_score:.4f}, "
            f"Test Acc: {test_score:.4f}"
        )

        early_stopping.update(val_score)
        if early_stopping.should_stop():
            break

        if val_score > best["val"]:
            print("ğŸŒ¸ New best epoch! ğŸŒ¸")
            best = {"val": val_score, "test": test_score, "epoch": epoch}
            best_time = time.time()

        print()

    if best_time is not None:
        total_time = best_time - start_time
        print(f"Time to best model: {total_time:.2f} seconds")
    else:
        print("No best model found.")

    return best


@torch.no_grad()
def evaluate(model, data, part: str, batch_size=32) -> Tuple[float, float, float]:
    model.eval()
    eval_batch_size = batch_size

    y_pred = []
    y_true = []
    total_samples = len(data[part]["y"])
    total_time = 0.0  # æ€»æ¨ç†æ—¶é—´ï¼ˆç§’ï¼‰
    latency_list = []  # è®°å½•æ¯ä¸ª batch çš„å»¶è¿Ÿ

    for batch_cpu in delu.iter_batches(data[part], eval_batch_size):
        batch = {k: v.to(device) for k, v in batch_cpu.items()}

        # é¢„çƒ­ï¼ˆé¿å…é¦–æ¬¡æ¨ç†çš„ CUDA åˆå§‹åŒ–å½±å“å»¶è¿Ÿï¼‰
        if len(latency_list) == 0:
            with autocast('cuda'):
                _ = apply_model(model, batch)

        # æ­£å¼æµ‹é‡æ—¶é—´
        start_time = time.time()
        with autocast('cuda'):
            output = apply_model(model, batch)
        torch.cuda.synchronize()  # ç¡®ä¿ CUDA æ“ä½œå®Œæˆ
        end_time = time.time()

        batch_time = end_time - start_time
        latency_list.append(batch_time)
        total_time += batch_time

        y_pred.append(output.cpu().numpy())
        y_true.append(batch["y"].cpu().numpy())

    y_pred = np.concatenate(y_pred, axis=0)
    y_true = np.concatenate(y_true, axis=0)

    y_pred_classes = np.argmax(y_pred, axis=1)
    y_true_classes = y_true
    score = sklearn.metrics.accuracy_score(y_true_classes, y_pred_classes)

    # è®¡ç®—ååé‡ï¼ˆæ ·æœ¬/ç§’ï¼‰
    throughput = total_samples / total_time if total_time > 0 else 0.0

    # è®¡ç®—å¹³å‡å»¶è¿Ÿï¼ˆç§’/æ ·æœ¬ï¼‰
    avg_latency_per_sample = total_time / total_samples if total_samples > 0 else 0.0

    # è®¡ç®—å¹³å‡å»¶è¿Ÿï¼ˆç§’/batchï¼‰
    avg_latency_per_batch = np.mean(latency_list) if latency_list else 0.0

    return score, throughput, avg_latency_per_sample


@torch.no_grad()
def evaluate_by_snr(
    model,
    data: Dict[str, Dict[str, torch.Tensor]],
    lbl: list,
    snrs: list,
    test_idx: list,
    classes: list,
    batch_size=32
):
    model.eval()

    test_x = data["test"]["x"]
    test_y = data["test"]["y"].cpu().numpy()  # ç¡®ä¿æ˜¯ numpy æ•°ç»„
    test_y_classes = np.argmax(test_y, axis=1)  # è½¬æ¢ä¸ºç±»åˆ«ç´¢å¼•

    test_y_pred = []

    n_samples_test = test_x.size(0)
    with autocast('cuda'):
        outputs = []
        for batch_cpu in delu.iter_batches({"x": test_x, "y": test_y}, batch_size=batch_size, shuffle=False):
            batch = {k: v.to(device) for k, v in batch_cpu.items()}
            output = apply_model(model, batch)
            outputs.append(output.cpu().numpy())
        y_pred_np = np.concatenate(outputs, axis=0)

    y_pred_classes = np.argmax(y_pred_np, axis=1)

    acc_per_snr = {}

    for snr in snrs:
        indices_of_snr = [
            i for i, global_idx in enumerate(test_idx)
            if lbl[global_idx][1] == snr
        ]
        pred_snr = y_pred_classes[indices_of_snr]
        true_snr = test_y_classes[indices_of_snr]
        if len(pred_snr) == 0:
            acc_per_snr[snr] = None
            continue
        correct = np.sum(pred_snr == true_snr)
        total = len(pred_snr)
        accuracy_snr = correct / total
        acc_per_snr[snr] = accuracy_snr

    print("\n===== Accuracy By SNR =====")
    for snr in sorted(acc_per_snr.keys()):
        acc_val = acc_per_snr[snr]
        if acc_val is None:
            print(f"SNR = {snr:>3}: No samples in test set.")
        else:
            print(f"SNR = {snr:>3}: Acc = {acc_val: .4f}")
    print("==========================\n")

    return acc_per_snr


def get_dataset_loader(dataset_version):
    """æ ¹æ®æ•°æ®é›†ç‰ˆæœ¬è¿”å›å¯¹åº”çš„åŠ è½½å‡½æ•°"""
    if dataset_version == '10a':
        from dataset.RML201610a import load_data
        return load_data, 'RML2016.10a.pkl'
    elif dataset_version == '10b':
        from dataset.RML201610b import load_data
        return load_data, 'RML2016.10b.dat'
    elif dataset_version == '04c':
        from dataset.RML201604c import load_data
        return load_data, 'RML2016.04c.pkl'


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser()
    parser.add_argument('--dataset', type=str, default='10b',
                        choices=['10a', '10b', '04c'], help='Dataset version')
    parser.add_argument('--data_path', type=str, default='./dataset',
                        help='Path to dataset directory')
    parser.add_argument('--n_epochs', type=int, default=1_000_000_000)
    parser.add_argument('--patience', type=int, default=16)
    parser.add_argument('--batch_size', type=int, default=1024)
    parser.add_argument('--accumulation_steps', type=int, default=1)
    parser.add_argument('--data_ratio', type=float, default=1.0)
    parser.add_argument('--lr', type=float, default=1e-3, help='åˆå§‹å­¦ä¹ ç‡')
    parser.add_argument('--lr_patience', type=int, default=5, help='å­¦ä¹ ç‡è°ƒåº¦å™¨çš„è€å¿ƒå€¼(å¤šå°‘ä¸ªepochæ— æ”¹å–„åé™å­¦ä¹ ç‡)')
    parser.add_argument('--lr_factor', type=float, default=0.5, help='å­¦ä¹ ç‡è¡°å‡ç³»æ•°')
    parser.add_argument('--device', type=str, default="cuda", choices=["cuda", "cpu"], help='è¿è¡Œè®¾å¤‡')
    parser.add_argument('--gpu', type=str, default="0", help='æŒ‡å®šè¦ä½¿ç”¨çš„ GPU è®¾å¤‡ç¼–å·ï¼Œå¦‚ "0" æˆ– "0,1,2"')
    return parser.parse_args()


def main():
    """ä¸»å‡½æ•°"""
    torch.cuda.empty_cache()
    gc.collect()

    args = parse_args()

    # è®¾ç½® CUDA_VISIBLE_DEVICES ç¯å¢ƒå˜é‡
    if args.device == "cuda":
        os.environ["CUDA_VISIBLE_DEVICES"] = args.gpu
        print(f"Using GPU(s): {args.gpu}")
    else:
        print("Using CPU")

    # è®¾ç½®è®¾å¤‡
    device = torch.device(args.device if torch.cuda.is_available() and args.device == "cuda" else "cpu")
    print(f"Using device: {device}")

    # è·å–æ•°æ®åŠ è½½å™¨å’Œå¯¹åº”æ–‡ä»¶å
    load_data_fn, data_file = get_dataset_loader(args.dataset)
    data_path = os.path.join(args.data_path, data_file)

    # åŠ è½½æ•°æ®
    data, n_classes, n_cont_features, mods, snrs, lbl, test_idx = \
        load_and_preprocess_data(load_data_fn, data_path, args.batch_size, args.data_ratio)

    model, optimizer, scheduler = define_model(n_cont_features, n_classes, args)
    print(f"Model device: {next(model.parameters()).device}")

    # è®¡ç®—æ¨¡å‹å¤æ‚åº¦
    dummy_input = torch.randn(1, n_cont_features).to(device)
    flops, params = profile(model, inputs=(dummy_input,))
    flops, params = clever_format([flops, params], "%.3f")
    print(f"Total Parameters: {params}")
    print(f"FLOPs per sample: {flops}")

    # è®­ç»ƒæ¨¡å‹
    best = train_model(
        model, optimizer, scheduler, data,
        n_epochs=args.n_epochs,
        patience=args.patience,
        batch_size=args.batch_size,
        accumulation_steps=args.accumulation_steps,
    )

    # æœ€ç»ˆè¯„ä¼°
    test_score, test_throughput, test_latency = evaluate(model, data, "test", args.batch_size)
    print(f'Test score after training: {test_score:.4f}')
    print(f'Throughput after training: {test_throughput:.2f} samples/second')
    print(f'Latency after training: {test_latency * 1000:.2f} ms/sample')

    print("\nResult:")
    print(best)

    # æŒ‰SNRè¯„ä¼°
    evaluate_by_snr(
        model=model,
        data=data,
        lbl=lbl,
        snrs=snrs,
        test_idx=test_idx,
        classes=mods,
        batch_size=args.batch_size
    )


if __name__ == "__main__":
    main()